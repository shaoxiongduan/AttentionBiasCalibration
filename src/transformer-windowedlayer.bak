import torch
from torch import Tensor
import torch.nn as nn
from torch.nn import Transformer, TransformerDecoderLayer
import math
from typing import Optional, Any, Union, Callable
from torch import functional as F



# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.
class PositionalEncoding(nn.Module):
    def __init__(self,
                 emb_size: int,
                 dropout: float,
                 maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        #pos_embedding = torch.zeros((maxlen, emb_size))
        #pos_embedding[:, 0::2] = torch.sin(pos * den)
        #pos_embedding[:, 1::2] = torch.cos(pos * den)

        # Try linear positional encoding:
        pos_embedding = torch.ones((maxlen, emb_size))
        for i in range(pos_embedding.size(0)):
            pos_embedding[i, :] = pos_embedding[i, :]*math.log(i+1)


        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)

    def forward(self, token_embedding: Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])

# helper Module to convert tensor of input indices into corresponding tensor of token embeddings
class TokenEmbedding(nn.Module):
    def __init__(self, vocab_size: int, emb_size):
        super(TokenEmbedding, self).__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size)
        self.emb_size = emb_size

    def forward(self, tokens: Tensor):
        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)

# Seq2Seq Network
class Seq2SeqTransformer(nn.Module):
    def __init__(self,
                 num_encoder_layers: int,
                 num_decoder_layers: int,
                 emb_size: int,
                 nhead: int,
                 vocab_size: int,
                 dim_feedforward: int = 512,
                 dropout: float = 0.1,
                 custom_encoder: Optional[Any] = None):
        super(Seq2SeqTransformer, self).__init__()
        self.transformer = Transformer(d_model=emb_size,
                                       nhead=nhead,
                                       num_encoder_layers=num_encoder_layers,
                                       num_decoder_layers=num_decoder_layers,
                                       dim_feedforward=dim_feedforward,
                                       dropout=dropout,
                                       custom_encoder=custom_encoder)
        self.generator = nn.Linear(emb_size, vocab_size)
        # The liner project layer, converting from emb_size to vocab_size
        self.tok_emb = TokenEmbedding(vocab_size, emb_size)
        self.positional_encoding = PositionalEncoding(
            emb_size, dropout=dropout)

    def forward(self, 
                src: Tensor, 
                tgt: Tensor, 
                src_mask: Optional[Tensor] = None, 
                tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None, 
                src_key_padding_mask: Optional[Tensor] = None,
                tgt_key_padding_mask: Optional[Tensor] = None, 
                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:
       
        src_emb = self.positional_encoding(self.tok_emb(src))
        tgt_emb = self.positional_encoding(self.tok_emb(tgt))
        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, memory_mask,
                                src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)
        return self.generator(outs)

    def encode(self, src: Tensor, src_mask: Tensor):
        return self.transformer.encoder(self.positional_encoding(
                            self.tok_emb(src)), src_mask)

    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,
                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,
                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:
    #def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):

        return self.transformer.decoder(self.positional_encoding(self.tok_emb(tgt)), memory,
                                        tgt_mask, memory_mask, tgt_key_padding_mask,
                                        memory_key_padding_mask)
   
    # Looks like we can use pytorch's Transformer directly


class IdentityEncoder(nn.Module):
    r"""IdentityEncoder is an encoder that simply passess input through. It is used here to 
        implement prefix LM.
    """
    def __init__(self):
        super().__init__()
        torch._C._log_api_usage_once(f"torch.nn.modules.{self.__class__.__name__}")

    def forward(
            self,
            src: Tensor,
            mask: Optional[Tensor] = None,
            src_key_padding_mask: Optional[Tensor] = None,
            is_causal: Optional[bool] = None) -> Tensor:
        r"""Pass the input through the encoder layers in turn.
            We have to implement the same interface as TransformerEncoder. But all arguments
            except src are ignored.

        Args:
            src: the sequence to the encoder (required).
            mask: the mask for the src sequence (optional).
            is_causal: If specified, applies a causal mask as mask (optional)
                and ignores attn_mask for computing scaled dot product attention.
                Default: ``False``.
            src_key_padding_mask: the mask for the src keys per batch (optional).
            
        Shape:
            see the docs in Transformer class.
        """
        #return src.clone()
        return src
        # TODO: WHich is right?  





'''
Inherited from pytorch's nn.Module.TransformerDecoderLayer and hacked to implement the windowed attention.

Usage:
    Create a TransformerEncoder with the customized decoder, and create the transformer model with it.


This is not working yet and may not be necessary.
'''

class WindowedAttentionDecoderLayer(TransformerDecoderLayer):
    r""" Everything is the same except for MHA
    
    
    Args:
        d_model: the number of expected features in the input (required).
        nhead: the number of heads in the multiheadattention models (required).
        dim_feedforward: the dimension of the feedforward network model (default=2048).
        dropout: the dropout value (default=0.1).
        activation: the activation function of the intermediate layer, can be a string
            ("relu" or "gelu") or a unary callable. Default: relu
        layer_norm_eps: the eps value in layer normalization components (default=1e-5).
        batch_first: If ``True``, then the input and output tensors are provided
            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).
        norm_first: if ``True``, layer norm is done prior to self attention, multihead
            attention and feedforward operations, respectively. Otherwise it's done after.
            Default: ``False`` (after).
        window_size: window_size

    Examples::
        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
        >>> memory = torch.rand(10, 32, 512)
        >>> tgt = torch.rand(20, 32, 512)
        >>> out = decoder_layer(tgt, memory)

    Alternatively, when ``batch_first`` is ``True``:
        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8, batch_first=True)
        >>> memory = torch.rand(32, 10, 512)
        >>> tgt = torch.rand(32, 20, 512)
        >>> out = decoder_layer(tgt, memory)
    """
    __constants__ = ['window_size']

    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,
                 activation: Union[str, Callable[[Tensor], Tensor]] = F.relu,
                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,
                 device=None, dtype=None, window_size = 2) -> None:
        super().__init__(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, 
                         batch_first, norm_first, device, dtype)
        self.window_size = window_size
        

    # self-attention block
    # Override to implement windowed attention
    def _sa_block(self, x: Tensor,
                  attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:

        


        x = self.self_attn(x, x, x,
                           attn_mask=attn_mask,
                           key_padding_mask=key_padding_mask,
                           is_causal=is_causal,
                           need_weights=False)[0]

        '''
        x is of shape :math:`(L, E)` when input is unbatched, :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` 
        when ``batch_first=True``, where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` 
        is the embedding dimension  ``embed_dim``. See docs of MultiheadAttention.   
        
        This is the self attention that is calculated on the (accumulated) output sequence. In our case, the most importent 
        token is the previous one. So we zero-out the other vectors.

        The following did NOT work! The returned vectors are weighted sums of the original value vectors AND go through softmax.
        Try using attn_mask.
        '''
        if self.batch_first:
            for i in range(x.size()[1]-self.window_size):
                x[:, i, :] = 0
        else:
            for i in range(x.size()[0]-self.window_size):
                x[i, :, :] = 0

        return self.dropout1(x)

    # multihead attention block
    # Override to implement windowed attention
    def _mha_block(self, x: Tensor, mem: Tensor,
                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:
        x = self.multihead_attn(x, mem, mem,
                                attn_mask=attn_mask,
                                key_padding_mask=key_padding_mask,
                                is_causal=is_causal,
                                need_weights=False)[0]
        '''
        x is of shape :math:`(L, E)` when input is unbatched, :math:`(L, N, E)` when ``batch_first=False`` or :math:`(N, L, E)` 
        when ``batch_first=True``, where :math:`L` is the target sequence length, :math:`N` is the batch size, and :math:`E` 
        is the embedding dimension  ``embed_dim``. See docs of MultiheadAttention.   

        x is length L, mem is length S. so shouldn't the output be :math:`(S, E)`, not :math:`(L, E)`?

        but x is accumulated, so it should be of the same shape as done in self attention

        Cross attention is attending to encoder outputs. The most relevant digit is the one that we are generating. 
        '''


        return self.dropout2(x)